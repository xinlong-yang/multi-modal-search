,Conference,Name,Author,School,abstract
0,CVPR,CLIPPO: Image-and-Language Understanding From Pixels Only ,"[u'Michael Tschannen (Google DeepMind) ', u' Basil Mustafa (Google) ', u' Neil Houlsby (Google)']",Google,"Multimodal models are becoming increasingly effective, in part due to unified components, such as the Transformer architecture. However, multimodal models still often consist of many task- and modality-specific pieces and training procedures. For example, CLIP (Radford et al., 2021) trains independent text and image towers via a contrastive loss. We explore an additional unification: the use of a pure pixel-based model to perform image, text, and multimodal tasks. Our model is trained with contrastive loss alone, so we call it CLIP-Pixels Only (CLIPPO). CLIPPO uses a single encoder that processes both regular images and text rendered as images. CLIPPO performs image-based tasks such as retrieval and zero-shot image classification almost as well as CLIP-style models, with half the number of parameters and no text-specific tower or embedding. When trained jointly via image-text contrastive learning and next-sentence contrastive learning, CLIPPO can perform well on natural language understanding tasks, without any word-level loss (language modelling or masked language modelling), outperforming pixel-based prior work. Surprisingly, CLIPPO can obtain good accuracy in visual question answering, simply by rendering the question and image together. Finally, we exploit the fact that CLIPPO does not require a tokenizer to show that it can achieve strong performance on multilingual multimodal retrieval without modifications. Code and pretrained models are available at https://github.com/google-research/big_vision."
1,CVPR,REVEAL: Retrieval-Augmented Visual-Language Pre-Training With Multi-Source Multimodal Knowledge Memory ,"[u'Ziniu Hu (None) ', u' Ahmet Iscen (Google) ', u' Chen Sun (Brown University) ', u' Zirui Wang (Google Brain) ', u' Kai-Wei Chang (University of California, Los Angeles) ', u' Yizhou Sun (University of California, Los Angeles) ', u' Cordelia Schmid (Inria / Google) ', u' David A. Ross (Google Research) ', u' Alireza Fathi (Google)']",Google,"In this paper, we propose an end-to-end Retrieval-Augmented Visual Language Model (REVEAL) that learns to encode world knowledge into a large-scale memory, and to retrieve from it to answer knowledge-intensive queries. REVEAL consists of four key components: the memory, the encoder, the retriever and the generator. The large-scale memory encodes various sources of multimodal world knowledge (e.g. image-text pairs, question answering pairs, knowledge graph triplets, etc.) via a unified encoder. The retriever finds the most relevant knowledge entries in the memory, and the generator fuses the retrieved knowledge with the input query to produce the output. A key novelty in our approach is that the memory, encoder, retriever and generator are all pre-trained end-to-end on a massive amount of data. Furthermore, our approach can use a diverse set of multimodal knowledge sources, which is shown to result in significant gains. We show that REVEAL achieves state-of-the-art results on visual question answering and image captioning."
2,CVPR,Connecting Vision and Language With Video Localized Narratives ,"[u'Paul Voigtlaender (None) ', u' Soravit Changpinyo (Google Research) ', u' Jordi Pont-Tuset (Google Research) ', u' Radu Soricut (Google) ', u' Vittorio Ferrari (Google)']",Google,"We propose Video Localized Narratives, a new form of multimodal video annotations connecting vision and language. In the original Localized Narratives, annotators speak and move their mouse simultaneously on an image, thus grounding each word with a mouse trace segment. However, this is challenging on a video. Our new protocol empowers annotators to tell the story of a video with Localized Narratives, capturing even complex events involving multiple actors interacting with each other and with several passive objects. We annotated 20k videos of the OVIS, UVO, and Oops datasets, totalling 1.7M words. Based on this data, we also construct new benchmarks for the video narrative grounding and video question answering tasks, and provide reference results from strong baseline models. Our annotations are available at https://google.github.io/video-localized-narratives/."
3,CVPR,Language-Guided Audio-Visual Source Separation via Trimodal Consistency ,"[u'Reuben Tan (Boston University) ', u' Arijit Ray (Boston University) ', u' Andrea Burns (Boston University) ', u' Bryan A. Plummer (None) ', u' Justin Salamon (Adobe Research) ', u' Oriol Nieto (Adobe Research) ', u' Bryan Russell (Adobe Research) ', u' Kate Saenko (Meta / Boston University)']",Meta / Boston University,"We propose a self-supervised approach for learning to perform audio source separation in videos based on natural language queries, using only unlabeled video and audio pairs as training data. A key challenge in this task is learning to associate the linguistic description of a sound-emitting object to its visual features and the corresponding components of the audio waveform, all without access to annotations during training. To overcome this challenge, we adapt off-the-shelf vision-language foundation models to provide pseudo-target supervision via two novel loss functions and encourage a stronger alignment between the audio, visual and natural language modalities. During inference, our approach can separate sounds given text, video and audio input, or given text and audio input alone. We demonstrate the effectiveness of our self-supervised approach on three audio-visual separation datasets, including MUSIC, SOLOS and AudioSet, where we outperform state-of-the-art strongly supervised approaches despite not using object detectors or text labels during training. Finally, we also include samples of our separated audios in the supplemental for reference."
4,CVPR,V2V4Real: A Real-World Large-Scale Dataset for Vehicle-to-Vehicle Cooperative Perception ,"[u'Runsheng Xu (University of California, Los Angeles) ', u' Xin Xia (University of California, Los Angeles) ', u' JINLONG LI (Cleveland State University) ', u' Hanzhao Li (University of California, Los Angeles) ', u' Shuo Zhang (University of California, Los Angeles) ', u' Zhengzhong Tu (University of Texas at Austin) ', u' Zonglin Meng (University of Wisconsin - Madison) ', u' Hao Xiang (University of California, Los Angeles) ', u' Xiaoyu Dong (Northwestern University, Northwestern University) ', u' Rui Song (Technical University of Munich) ', u' Hongkai Yu (Cleveland State University) ', u' Bolei Zhou (University of California, Los Angeles) ', u' Jiaqi Ma (University of California, Los Angeles)']","University of California, Los Angeles","Modern perception systems of autonomous vehicles are known to be sensitive to occlusions and lack the capability of long perceiving range. It has been one of the key bottlenecks that prevents Level 5 autonomy. Recent research has demonstrated that the Vehicle-to-Vehicle (V2V) cooperative perception system has great potential to revolutionize the autonomous driving industry. However, the lack of a real-world dataset hinders the progress of this field. To facilitate the development of cooperative perception, we present V2V4Real, the first large-scale real-world multi-modal dataset for V2V perception. The data is collected by two vehicles equipped with multi-modal sensors driving together through diverse scenarios. Our V2V4Real dataset covers a driving area of 410 km, comprising 20K LiDAR frames, 40K RGB frames, 240K annotated 3D bounding boxes for 5 classes, and HDMaps that cover all the driving routes. V2V4Real introduces three perception tasks, including cooperative 3D object detection, cooperative 3D object tracking, and Sim2Real domain adaptation for cooperative perception. We provide comprehensive benchmarks of recent cooperative perception algorithms on three tasks. The V2V4Real dataset can be found at research.seas.ucla.edu/mobility-lab/v2v4real/."
5,CVPR,Modality-Invariant Visual Odometry for Embodied Vision ,"[u'Marius Memmel (University of Washington) ', u' Roman Bachmann (EPFL - EPF Lausanne) ', u' Amir Zamir (Swiss Federal Institute of Technology (EPFL))']",Swiss Federal Institute of Technology (EPFL,"Effectively localizing an agent in a realistic, noisy setting is crucial for many embodied vision tasks. Visual Odometry (VO) is a practical substitute for unreliable GPS and compass sensors, especially in indoor environments. While SLAM-based methods show a solid performance without large data requirements, they are less flexible and robust w.r.t. to noise and changes in the sensor suite compared to learning-based approaches. Recent deep VO models, however, limit themselves to a fixed set of input modalities, e.g., RGB and depth, while training on millions of samples. When sensors fail, sensor suites change, or modalities are intentionally looped out due to available resources, e.g., power consumption, the models fail catastrophically. Furthermore, training these models from scratch is even more expensive without simulator access or suitable existing models that can be fine-tuned. While such scenarios get mostly ignored in simulation, they commonly hinder a model's reusability in real-world applications. We propose a Transformer-based modality-invariant VO approach that can deal with diverse or changing sensor suites of navigation agents. Our model outperforms previous methods while training on only a fraction of the data. We hope this method opens the door to a broader range of real-world applications that can benefit from flexible and learned VO models."
6,CVPR,Improving Selective Visual Question Answering by Learning From Your Peers ,"[u'Corentin Dancette (Sorbonne Universite) ', u' Spencer Whitehead (FAIR, Meta AI) ', u' Rishabh Maheshwary (Facebook) ', u' Ramakrishna Vedantam (Facebook) ', u' Stefan Scherer (Facebook) ', u' Xinlei Chen (Facebook) ', u' Matthieu Cord (None) ', u' Marcus Rohrbach (Facebook)']",Facebook,"Despite advances in Visual Question Answering (VQA), the ability of models to assess their own correctness remains underexplored. Recent work has shown that VQA models, out-of-the-box, can have difficulties abstaining from answering when they are wrong. The option to abstain, also called Selective Prediction, is highly relevant when deploying systems to users who must trust the system's output (e.g., VQA assistants for users with visual impairments). For such scenarios, abstention can be especially important as users may provide out-of-distribution (OOD) or adversarial inputs that make incorrect answers more likely. In this work, we explore Selective VQA in both in-distribution (ID) and OOD scenarios, where models are presented with mixtures of ID and OOD data. The goal is to maximize the number of questions answered while minimizing the risk of error on those questions. We propose a simple yet effective Learning from Your Peers (LYP) approach for training multimodal selection functions for making abstention decisions. Our approach uses predictions from models trained on distinct subsets of the training data as targets for optimizing a Selective VQA model. It does not require additional manual labels or held-out data and provides a signal for identifying examples that are easy/difficult to generalize to. In our extensive evaluations, we show this benefits a number of models across different architectures and scales. Overall, for ID, we reach 32.92% in the selective prediction metric coverage at 1% risk of error (C@1%) which doubles the previous best coverage of 15.79% on this task. For mixed ID/OOD, using models' softmax confidences for abstention decisions performs very poorly, answering <5% of questions at 1% risk of error even when faced with only 10% OOD examples, but a learned selection function with LYP can increase that to 25.38% C@1%."
7,CVPR,Language-Guided Music Recommendation for Video via Prompt Analogies ,"[u'Daniel McKee (University of Illinois, Urbana Champaign) ', u' Justin Salamon (Adobe Research) ', u' Josef Sivic (Czech Technical University in Prague) ', u' Bryan Russell (Adobe Research)']",Adobe Research,"We propose a method to recommend music for an input video while allowing a user to guide music selection with free-form natural language. A key challenge of this problem setting is that existing music video datasets provide the needed (video, music) training pairs, but lack text descriptions of the music. This work addresses this challenge with the following three contributions. First, we propose a text-synthesis approach that relies on an analogy-based prompting procedure to generate natural language music descriptions from a large-scale language model (BLOOM-176B) given pre-trained music tagger outputs and a small number of human text descriptions. Second, we use these synthesized music descriptions to train a new trimodal model, which fuses text and video input representations to query music samples. For training, we introduce a text dropout regularization mechanism which we show is critical to model performance. Our model design allows for the retrieved music audio to agree with the two input modalities by matching visual style depicted in the video and musical genre, mood, or instrumentation described in the natural language query. Third, to evaluate our approach, we collect a testing dataset for our problem by annotating a subset of 4k clips from the YT8M-MusicVideo dataset with natural language music descriptions which we make publicly available. We show that our approach can match or exceed the performance of prior methods on video-to-music retrieval while significantly improving retrieval accuracy when using text guidance."
8,CVPR,Query-Centric Trajectory Prediction ,"[u'Zikang Zhou (City University of Hong Kong) ', u' Jianping Wang (City University of Hong Kong) ', u' Yung-Hui Li (Hon Hai Research Institute) ', u' Yu-Kai Huang (CMU, Carnegie Mellon University)']","CMU, Carnegie Mellon University","Predicting the future trajectories of surrounding agents is essential for autonomous vehicles to operate safely. This paper presents QCNet, a modeling framework toward pushing the boundaries of trajectory prediction. First, we identify that the agent-centric modeling scheme used by existing approaches requires re-normalizing and re-encoding the input whenever the observation window slides forward, leading to redundant computations during online prediction. To overcome this limitation and achieve faster inference, we introduce a query-centric paradigm for scene encoding, which enables the reuse of past computations by learning representations independent of the global spacetime coordinate system. Sharing the invariant scene features among all target agents further allows the parallelism of multi-agent trajectory decoding. Second, even given rich encodings of the scene, existing decoding strategies struggle to capture the multimodality inherent in agents' future behavior, especially when the prediction horizon is long. To tackle this challenge, we first employ anchor-free queries to generate trajectory proposals in a recurrent fashion, which allows the model to utilize different scene contexts when decoding waypoints at different horizons. A refinement module then takes the trajectory proposals as anchors and leverages anchor-based queries to refine the trajectories further. By supplying adaptive and high-quality anchors to the refinement module, our query-based decoder can better deal with the multimodality in the output of trajectory prediction. Our approach ranks 1st on Argoverse 1 and Argoverse 2 motion forecasting benchmarks, outperforming all methods on all main metrics by a large margin. Meanwhile, our model can achieve streaming scene encoding and parallel multi-agent decoding thanks to the query-centric design ethos."
9,CVPR,Learning Visual Representations via Language-Guided Sampling ,"[u'Mohamed El Banani (University of Michigan) ', u' Karan Desai (None) ', u' Justin Johnson (University of Michigan)']",University of Michigan,"Although an object may appear in numerous contexts, we often describe it in a limited number of ways. Language allows us to abstract away visual variation to represent and communicate concepts. Building on this intuition, we propose an alternative approach to visual representation learning: using language similarity to sample semantically similar image pairs for contrastive learning. Our approach diverges from image-based contrastive learning by sampling view pairs using language similarity instead of hand-crafted augmentations or learned clusters. Our approach also differs from image-text contrastive learning by relying on pre-trained language models to guide the learning rather than directly minimizing a cross-modal loss. Through a series of experiments, we show that language-guided learning yields better features than image-based and image-text representation learning approaches."
10,CVPR,3D Concept Learning and Reasoning From Multi-View Images ,"[u'Yining Hong () ', u' Chunru Lin (Shanghai Jiaotong University) ', u' Yilun Du (Massachusetts Institute of Technology) ', u' Zhenfang Chen (MIT-IBM Watson AI lab) ', u' Joshua B. Tenenbaum (MIT) ', u' Chuang Gan (MIT-IBM Watson AI Lab)']",MIT-IBM Watson AI Lab,"Humans are able to accurately reason in 3D by gathering multi-view observations of the surrounding world. Inspired by this insight, we introduce a new large-scale benchmark for 3D multi-view visual question answering (3DMV-VQA). This dataset is collected by an embodied agent actively moving and capturing RGB images in an environment using the Habitat simulator. In total, it consists of approximately 5k scenes, 600k images, paired with 50k questions. We evaluate various state-of-the-art models for visual reasoning on our benchmark and find that they all perform poorly. We suggest that a principled approach for 3D reasoning from multi-view images should be to infer a compact 3D representation of the world from the multi-view images, which is further grounded on open-vocabulary semantic concepts, and then to execute reasoning on these 3D representations. As the first step towards this approach, we propose a novel 3D concept learning and reasoning (3D-CLR) framework that seamlessly combines these components via neural fields, 2D pre-trained vision-language models, and neural reasoning operators. Experimental results suggest that our framework outperforms baseline models by a large margin, but the challenge remains largely unsolved. We further perform an in-depth analysis of the challenges and highlight potential future directions."
11,CVPR,EC2: Emergent Communication for Embodied Control ,"[u'Yao Mu (The University of Hong Kong) ', u' Shunyu Yao (Princeton University) ', u' Mingyu Ding (UC Berkeley) ', u' Ping Luo (The University of Hong Kong) ', u' Chuang Gan (MIT-IBM Watson AI Lab)']",MIT-IBM Watson AI Lab,"Embodied control requires agents to leverage multi-modal pre-training to quickly learn how to act in new environments, where video demonstrations contain visual and motion details needed for low-level perception and control, and language instructions support generalization with abstract, symbolic structures. While recent approaches apply contrastive learning to force alignment between the two modalities, we hypothesize better modeling their complementary differences can lead to more holistic representations for downstream adaption. To this end, we propose Emergent Communication for Embodied Control (EC^2), a novel scheme to pre-train video-language representations for few-shot embodied control. The key idea is to learn an unsupervised ""language"" of videos via emergent communication, which bridges the semantics of video details and structures of natural language. We learn embodied representations of video trajectories, emergent language, and natural language using a language model, which is then used to finetune a lightweight policy network for downstream control. Through extensive experiments in Metaworld and Franka Kitchen embodied benchmarks, EC^2 is shown to consistently outperform previous contrastive learning methods for both videos and texts as task inputs. Further ablations confirm the importance of the emergent language, which is beneficial for both video and language learning, and significantly superior to using pre-trained video captions. We also present a quantitative and qualitative analysis of the emergent language and discuss future directions toward better understanding and leveraging emergent communication in embodied tasks."
12,CVPR,Understanding and Constructing Latent Modality Structures in Multi-Modal Representation Learning ,"[u'Qian Jiang (University of Illinois, Urbana Champaign) ', u' Changyou Chen (State University of New York, Buffalo) ', u' Han Zhao (University of Illinois, Urbana Champaign) ', u' Liqun Chen (Amazon) ', u' Qing Ping (Amazon) ', u' Son Dinh Tran (Amazon) ', u' Yi Xu (Amazon) ', u' Belinda Zeng (Amazon) ', u' Trishul Chilimbi (Department of Computer Science, University of Wisconsin - Madison)']","Department of Computer Science, University of Wisconsin - Madison","Contrastive loss has been increasingly used in learning representations from multiple modalities. In the limit, the nature of the contrastive loss encourages modalities to exactly match each other in the latent space. Yet it remains an open question how the modality alignment affects the downstream task performance. In this paper, based on an information-theoretic argument, we first prove that exact modality alignment is sub-optimal in general for downstream prediction tasks. Hence we advocate that the key of better performance lies in meaningful latent modality structures instead of perfect modality alignment. To this end, we propose three general approaches to construct latent modality structures. Specifically, we design 1) a deep feature separation loss for intra-modality regularization; 2) a Brownian-bridge loss for inter-modality regularization; and 3) a geometric consistency loss for both intra- and inter-modality regularization. Extensive experiments are conducted on two popular multi-modal representation learning frameworks: the CLIP-based two-tower model and the ALBEF-based fusion model. We test our model on a variety of tasks including zero/few-shot image classification, image-text retrieval, visual question answering, visual reasoning, and visual entailment. Our method achieves consistent improvements over existing methods, demonstrating the effectiveness and generalizability of our proposed approach on latent modality structure regularization."
13,CVPR,Novel-View Acoustic Synthesis ,"[u'Changan Chen (University of Texas at Austin) ', u' Alexander Richard (Reality Labs Research, Meta) ', u' Roman Shapovalov (Meta) ', u' Vamsi Krishna Ithapu (Facebook Reality Labs) ', u' Natalia Neverova (Facebook AI Research) ', u' Kristen Grauman (University of Texas at Austin) ', u' Andrea Vedaldi (University of Oxford)']",University of Oxford,"We introduce the novel-view acoustic synthesis (NVAS) task: given the sight and sound observed at a source viewpoint, can we synthesize the sound of that scene from an unseen target viewpoint? We propose a neural rendering approach: Visually-Guided Acoustic Synthesis (ViGAS) network that learns to synthesize the sound of an arbitrary point in space by analyzing the input audio-visual cues. To benchmark this task, we collect two first-of-their-kind large-scale multi-view audio-visual datasets, one synthetic and one real. We show that our model successfully reasons about the spatial cues and synthesizes faithful audio on both datasets. To our knowledge, this work represents the very first formulation, dataset, and approach to solve the novel-view acoustic synthesis task, which has exciting potential applications ranging from AR/VR to art and design. Unlocked by this work, we believe that the future of novel-view synthesis is in multi-modal learning from videos."
14,CVPR,AVFormer: Injecting Vision Into Frozen Speech Models for Zero-Shot AV-ASR ,"[u'Paul Hongsuck Seo (Google) ', u' Arsha Nagrani (Google ) ', u' Cordelia Schmid (Inria / Google)']",Inria / Google,"Audiovisual automatic speech recognition (AV-ASR) aims to improve the robustness of a speech recognition system by incorporating visual information. Training fully supervised multimodal models for this task from scratch, however is limited by the need for large labelled audiovisual datasets (in each downstream domain of interest). We present AVFormer, a simple method for augmenting audioonly models with visual information, at the same time performing lightweight domain adaptation. We do this by (i) injecting visual embeddings into a frozen ASR model using lightweight trainable adaptors. We show that these can be trained on a small amount of weakly labelled video data with minimum additional training time and parameters. (ii) We also introduce a simple curriculum scheme during training which we show is crucial to enable the model to jointly process audio and visual information effectively; and finally (iii) we show that our model achieves state of the art zero-shot results on three different AV-ASR benchmarks (How2, VisSpeech and Ego4D), while also crucially preserving decent performance on traditional audio-only speech recognition benchmarks (LibriSpeech). Qualitative results show that our model effectively leverages visual information for robust speech recognition."
15,CVPR,Mask-Free OVIS: Open-Vocabulary Instance Segmentation Without Manual Mask Annotations ,"[u'Vibashan VS (Johns Hopkins University) ', u' Ning Yu (Salesforce Research) ', u' Chen Xing (SalesForce.com) ', u' Can Qin (Northeastern University) ', u' Mingfei Gao (Apple) ', u' Juan Carlos Niebles (Salesforce Research) ', u' Vishal M. Patel (Johns Hopkins University) ', u' Ran Xu (SalesForce.com)']",SalesForce.com,"Existing instance segmentation models learn task-specific information using manual mask annotations from base (training) categories. These mask annotations require tremendous human effort, limiting the scalability to annotate novel (new) categories. To alleviate this problem, Open-Vocabulary (OV) methods leverage large-scale image-caption pairs and vision-language models to learn novel categories. In summary, an OV method learns task-specific information using strong supervision from base annotations and novel category information using weak supervision from image-captions pairs. This difference between strong and weak supervision leads to overfitting on base categories, resulting in poor generalization towards novel categories. In this work, we overcome this issue by learning both base and novel categories from pseudo-mask annotations generated by the vision-language model in a weakly supervised manner using our proposed Mask-free OVIS pipeline. Our method automatically generates pseudo-mask annotations by leveraging the localization ability of a pre-trained vision-language model for objects present in image-caption pairs. The generated pseudo-mask annotations are then used to supervise an instance segmentation model, freeing the entire pipeline from any labour-expensive instance-level annotations and overfitting. Our extensive experiments show that our method trained with just pseudo-masks significantly improves the mAP scores on the MS-COCO dataset and OpenImages dataset compared to the recent state-of-the-art methods trained with manual masks. Codes and models are provided in https://vibashan.github.io/ovis-web/."
16,CVPR,Pix2map: Cross-Modal Retrieval for Inferring Street Maps From Images ,"[u'Xindi Wu (Princeton University) ', u' KwunFung Lau (CMU, Carnegie Mellon University) ', u' Francesco Ferroni () ', u' Aljo\u0161a O\u0161ep (Carnegie Mellon University) ', u' Deva Ramanan (Carnegie Mellon University)']",Carnegie Mellon University,"Self-driving vehicles rely on urban street maps for autonomous navigation. In this paper, we introduce Pix2Map, a method for inferring urban street map topology directly from ego-view images, as needed to continually update and expand existing maps. This is a challenging task, as we need to infer a complex urban road topology directly from raw image data. The main insight of this paper is that this problem can be posed as cross-modal retrieval by learning a joint, cross-modal embedding space for images and existing maps, represented as discrete graphs that encode the topological layout of the visual surroundings. We conduct our experimental evaluation using the Argoverse dataset and show that it is indeed possible to accurately retrieve street maps corresponding to both seen and unseen roads solely from image data. Moreover, we show that our retrieved maps can be used to update or expand existing maps and even show proof-of-concept results for visual localization and image retrieval from spatial graphs."
17,CVPR,Improving Zero-Shot Generalization and Robustness of Multi-Modal Models ,"[u'Yunhao Ge (University of Southern California) ', u' Jie Ren (Google) ', u' Andrew Gallagher (Google) ', u' Yuxiao Wang (Google) ', u' Ming-Hsuan Yang (University of California at Merced) ', u' Hartwig Adam (Google Research) ', u' Laurent Itti (USC) ', u' Balaji Lakshminarayanan (Google Brain) ', u' Jiaping Zhao (Google)']",Google,"Multi-modal image-text models such as CLIP and LiT have demonstrated impressive performance on image classification benchmarks and their zero-shot generalization ability is particularly exciting. While the top-5 zero-shot accuracies of these models are very high, the top-1 accuracies are much lower (over 25% gap in some cases). We investigate the reasons for this performance gap and find that many of the failure cases are caused by ambiguity in the text prompts. First, we develop a simple and efficient zero-shot post-hoc method to identify images whose top-1 prediction is likely to be incorrect, by measuring consistency of the predictions w.r.t. multiple prompts and image transformations. We show that our procedure better predicts mistakes, outperforming the popular max logit baseline on selective prediction tasks. Next, we propose a simple and efficient way to improve accuracy on such uncertain images by making use of the WordNet hierarchy; specifically we augment the original class by incorporating its parent and children from the semantic label hierarchy, and plug the augmentation into text prompts. We conduct experiments on both CLIP and LiT models with five different ImageNet- based datasets. For CLIP, our method improves the top-1 accuracy by 17.13% on the uncertain subset and 3.6% on the entire ImageNet validation set. We also show that our method improves across ImageNet shifted datasets, four other datasets, and other model architectures such as LiT. Our proposed method is hyperparameter-free, requires no additional model training and can be easily scaled to other large multi-modal architectures. Code is available at https://github.com/gyhandy/Hierarchy-CLIP."
18,CVPR,Scaling Language-Image Pre-Training via Masking ,"[u'Yanghao Li (Facebook) ', u' Haoqi Fan (Facebook AI Research) ', u' Ronghang Hu (Meta AI) ', u' Christoph Feichtenhofer (Facebook) ', u' Kaiming He (Facebook AI Research)']",Facebook AI Research,"We present Fast Language-Image Pre-training (FLIP), a simple and more efficient method for training CLIP. Our method randomly masks out and removes a large portion of image patches during training. Masking allows us to learn from more image-text pairs given the same wall-clock time and contrast more samples per iteration with similar memory footprint. It leads to a favorable trade-off between accuracy and training time. In our experiments on 400 million image-text pairs, FLIP improves both accuracy and speed over the no-masking baseline. On a large diversity of downstream tasks, FLIP dominantly outperforms the CLIP counterparts trained on the same data. Facilitated by the speedup, we explore the scaling behavior of increasing the model size, data size, or training length, and report encouraging results and comparisons. We hope that our work will foster future research on scaling vision-language learning."
19,CVPR,MetaCLUE: Towards Comprehensive Visual Metaphors Research ,"[u'Arjun R. Akula (Google) ', u' Brendan Driscoll (University of Michigan - Ann Arbor) ', u' Pradyumna Narayana (Google) ', u' Soravit Changpinyo (Google Research) ', u' Zhiwei Jia (University of California, San Diego) ', u' Suyash Damle (Google) ', u' Garima Pruthi (Indian Institute of Technology, Guwahati) ', u' Sugato Basu (Google) ', u' Leonidas Guibas (Stanford University) ', u' William Freeman (MIT and Google) ', u' Yuanzhen Li (Massachusetts Institute of Technology) ', u' Varun Jampani (Google Research)']",Google Research,"Creativity is an indispensable part of human cognition and also an inherent part of how we make sense of the world. Metaphorical abstraction is fundamental in communicating creative ideas through nuanced relationships between abstract concepts such as feelings. While computer vision benchmarks and approaches predominantly focus on understanding and generating literal interpretations of images, metaphorical comprehension of images remains relatively unexplored. Towards this goal, we introduce MetaCLUE, a set of vision tasks on visual metaphor. We also collect high-quality and rich metaphor annotations (abstract objects, concepts, relationships along with their corresponding object boxes) as there do not exist any datasets that facilitate the evaluation of these tasks. We perform a comprehensive analysis of state-of-the-art models in vision and language based on our annotations, highlighting strengths and weaknesses of current approaches in visual metaphor Classification, Localization, Understanding (retrieval, question answering, captioning) and gEneration (text-to-image synthesis) tasks. We hope this work provides a concrete step towards systematically developing AI systems with human-like creative capabilities. Project page: https://metaclue.github.io"
20,CVPR,ImageBind: One Embedding Space To Bind Them All ,"[u'Rohit Girdhar (Meta) ', u' Alaaeldin El-Nouby (University of Guelph) ', u' Zhuang Liu (FAIR, Meta AI) ', u' Mannat Singh (Meta AI) ', u' Kalyan Vasudev Alwala (Facebook) ', u' Armand Joulin (Facebook) ', u' Ishan Misra (Facebook)']",Facebook,"We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications 'out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate vision models for visual and non-visual tasks."
21,CVPR,OmniMAE: Single Model Masked Pretraining on Images and Videos ,"[u'Rohit Girdhar (Meta) ', u' Alaaeldin El-Nouby (University of Guelph) ', u' Mannat Singh (Meta AI) ', u' Kalyan Vasudev Alwala (Facebook) ', u' Armand Joulin (Facebook) ', u' Ishan Misra (Facebook)']",Facebook,"Transformer-based architectures have become competitive across a variety of visual domains, most notably images and videos. While prior work studies these modalities in isolation, having a common architecture suggests that one can train a single unified model for multiple visual modalities. Prior attempts at unified modeling typically use architectures tailored for vision tasks, or obtain worse performance compared to single modality models. In this work, we show that masked autoencoding can be used to train a simple Vision Transformer on images and videos, without requiring any labeled data. This single model learns visual representations that are comparable to or better than single-modality representations on both image and video benchmarks, while using a much simpler architecture. Furthermore, this model can be learned by dropping 90% of the image and 95% of the video patches, enabling extremely fast training of huge model architectures. In particular, we show that our single ViT-Huge model can be finetuned to achieve 86.6% on ImageNet and 75.5% on the challenging Something Something-v2 video benchmark, setting a new state-of-the-art."
22,CVPR,VILA: Learning Image Aesthetics From User Comments With Vision-Language Pretraining ,"[u'Junjie Ke (None) ', u' Keren Ye (Google) ', u' Jiahui Yu (Google Brain) ', u' Yonghui Wu (, University of California, Riverside) ', u' Peyman Milanfar (Peyman Milanfar) ', u' Feng Yang (Google Research)']",Google Research,"Assessing the aesthetics of an image is challenging, as it is influenced by multiple factors including composition, color, style, and high-level semantics. Existing image aesthetic assessment (IAA) methods primarily rely on human-labeled rating scores, which oversimplify the visual aesthetic information that humans perceive. Conversely, user comments offer more comprehensive information and are a more natural way to express human opinions and preferences regarding image aesthetics. In light of this, we propose learning image aesthetics from user comments, and exploring vision-language pretraining methods to learn multimodal aesthetic representations. Specifically, we pretrain an image-text encoder-decoder model with image-comment pairs, using contrastive and generative objectives to learn rich and generic aesthetic semantics without human labels. To efficiently adapt the pretrained model for downstream IAA tasks, we further propose a lightweight rank-based adapter that employs text as an anchor to learn the aesthetic ranking concept. Our results show that our pretrained aesthetic vision-language model outperforms prior works on image aesthetic captioning over the AVA-Captions dataset, and it has powerful zero-shot capability for aesthetic tasks such as zero-shot style classification and zero-shot IAA, surpassing many supervised baselines. With only minimal finetuning parameters using the proposed adapter module, our model achieves state-of-the-art IAA performance over the AVA dataset."
23,CVPR,SCADE: NeRFs from Space Carving With Ambiguity-Aware Depth Estimates ,"[u'Mikaela Angelina Uy (Stanford University) ', u' Ricardo Martin-Brualla (Google) ', u' Leonidas Guibas (Stanford University) ', u' Ke Li (Simon Fraser University)']",Simon Fraser University,"Neural radiance fields (NeRFs) have enabled high fidelity 3D reconstruction from multiple 2D input views. However, a well-known drawback of NeRFs is the less-than-ideal performance under a small number of views, due to insufficient constraints enforced by volumetric rendering. To address this issue, we introduce SCADE, a novel technique that improves NeRF reconstruction quality on sparse, unconstrained input views for in-the-wild indoor scenes. To constrain NeRF reconstruction, we leverage geometric priors in the form of per-view depth estimates produced with state-of-the-art monocular depth estimation models, which can generalize across scenes. A key challenge is that monocular depth estimation is an ill-posed problem, with inherent ambiguities. To handle this issue, we propose a new method that learns to predict, for each view, a continuous, multimodal distribution of depth estimates using conditional Implicit Maximum Likelihood Estimation (cIMLE). In order to disambiguate exploiting multiple views, we introduce an original space carving loss that guides the NeRF representation to fuse multiple hypothesized depth maps from each view and distill from them a common geometry that is consistent with all views. Experiments show that our approach enables higher fidelity novel view synthesis from sparse views. Our project page can be found at https://scade-spacecarving-nerfs.github.io."
24,CVPR,Open-Vocabulary Semantic Segmentation With Mask-Adapted CLIP ,"[u'Feng Liang (The University of Texas at Austin) ', u' Bichen Wu (Facebook) ', u' Xiaoliang Dai (Facebook) ', u' Kunpeng Li (Meta) ', u' Yinan Zhao (Facebook) ', u' Hang Zhang (Cruise) ', u' Peizhao Zhang (Facebook) ', u' Peter Vajda (Facebook) ', u' Diana Marculescu (The University of Texas at Austin)']",The University of Texas at Austin,"Open-vocabulary semantic segmentation aims to segment an image into semantic regions according to text descriptions, which may not have been seen during training. Recent two-stage methods first generate class-agnostic mask proposals and then leverage pre-trained vision-language models, e.g., CLIP, to classify masked regions. We identify the performance bottleneck of this paradigm to be the pre-trained CLIP model, since it does not perform well on masked images. To address this, we propose to finetune CLIP on a collection of masked image regions and their corresponding text descriptions. We collect training data by mining an existing image-caption dataset (e.g., COCO Captions), using CLIP to match masked image regions to nouns in the image captions. Compared with the more precise and manually annotated segmentation labels with fixed classes (e.g., COCO-Stuff), we find our noisy but diverse dataset can better retain CLIP's generalization ability. Along with finetuning the entire model, we utilize the ""blank"" areas in masked images using a method we dub mask prompt tuning. Experiments demonstrate mask prompt tuning brings significant improvement without modifying any weights of CLIP, and it can further improve a fully finetuned model. In particular, when trained on COCO and evaluated on ADE20K-150, our best model achieves 29.6% mIoU, which is +8.5% higher than the previous state-of-the-art. For the first time, open-vocabulary generalist models match the performance of supervised specialist models in 2017 without dataset-specific adaptations."
25,CVPR,Fusing Pre-Trained Language Models With Multimodal Prompts Through Reinforcement Learning ,"[u'Youngjae Yu (Yonsei University) ', u' Jiwan Chung (Yonsei University) ', u' Heeseung Yun (Seoul National University) ', u' Jack Hessel (Allen Institute for Artificial Intelligence) ', u' Jae Sung Park (University of Washington) ', u' Ximing Lu (Department of Computer Science, University of Washington) ', u' Rowan Zellers (OpenAI) ', u' Prithviraj Ammanabrolu (Allen Institute for Artificial Intelligence) ', u' Ronan Le Bras (Allen Institute for Artificial Intelligence) ', u' Gunhee Kim (Seoul National University) ', u' Yejin Choi (Computer Science and Engineering, University of Washington)']","Computer Science and Engineering, University of Washington","Language models are capable of commonsense reasoning: while domain-specific models can learn from explicit knowledge (e.g. commonsense graphs [6], ethical norms [25]), and larger models like GPT-3 manifest broad commonsense reasoning capacity. Can their knowledge be extended to multimodal inputs such as images and audio without paired domain data? In this work, we propose ESPER (Extending Sensory PErception with Reinforcement learning) which enables text-only pretrained models to address multimodal tasks such as visual commonsense reasoning. Our key novelty is to use reinforcement learning to align multimodal inputs to language model generations without direct supervision: for example, our reward optimization relies only on cosine similarity derived from CLIP and requires no additional paired (image, text) data. Experiments demonstrate that ESPER outperforms baselines and prior work on a variety of multimodal text generation tasks ranging from captioning to commonsense reasoning; these include a new benchmark we collect and release, the ESP dataset, which tasks models with generating the text of several different domains for each image. Our code and data are publicly released at https://github.com/JiwanChung/esper."
26,CVPR,TOPLight: Lightweight Neural Networks With Task-Oriented Pretraining for Visible-Infrared Recognition ,"[u'Hao Yu (Nanjing University of Information Science and Technology) ', u' Xu Cheng (Nanjing University of Information Science and Technology) ', u' Wei Peng (Stanford University)']",Stanford University,"Visible-infrared recognition (VI recognition) is a challenging task due to the enormous visual difference across heterogeneous images. Most existing works achieve promising results by transfer learning, such as pretraining on the ImageNet, based on advanced neural architectures like ResNet and ViT. However, such methods ignore the negative influence of the pretrained colour prior knowledge, as well as their heavy computational burden makes them hard to deploy in actual scenarios with limited resources. In this paper, we propose a novel task-oriented pretrained lightweight neural network (TOPLight) for VI recognition. Specifically, the TOPLight method simulates the domain conflict and sample variations with the proposed fake domain loss in the pretraining stage, which guides the network to learn how to handle those difficulties, such that a more general modality-shared feature representation is learned for the heterogeneous images. Moreover, an effective fine-grained dependency reconstruction module (FDR) is developed to discover substantial pattern dependencies shared in two modalities. Extensive experiments on VI person re-identification and VI face recognition datasets demonstrate the superiority of the proposed TOPLight, which significantly outperforms the current state of the arts while demanding fewer computational resources."
27,CVPR,Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning With Multimodal Models ,"[u'Zhiqiu Lin (Carnegie Mellon University) ', u' Samuel Yu (Carnegie Mellon University) ', u' Zhiyi Kuang (None) ', u' Deepak Pathak (Carnegie Mellon University) ', u' Deva Ramanan (Carnegie Mellon University)']",Carnegie Mellon University,"The ability to quickly learn a new task with minimal instruction - known as few-shot learning - is a central aspect of intelligent agents. Classical few-shot benchmarks make use of few-shot samples from a single modality, but such samples may not be sufficient to characterize an entire concept class. In contrast, humans use cross-modal information to learn new concepts efficiently. In this work, we demonstrate that one can indeed build a better visual dog classifier by reading about dogs and listening to them bark. To do so, we exploit the fact that recent multimodal foundation models such as CLIP are inherently cross-modal, mapping different modalities to the same representation space. Specifically, we propose a simple cross-modal adaptation approach that learns from few-shot examples spanning different modalities. By repurposing class names as additional one-shot training samples, we achieve SOTA results with an embarrassingly simple linear classifier for vision-language adaptation. Furthermore, we show that our approach can benefit existing methods such as prefix tuning and classifier ensembling. Finally, to explore other modalities beyond vision and language, we construct the first (to our knowledge) audiovisual few-shot benchmark and use cross-modal training to improve the performance of both image and audio classification. We hope our success can inspire future works to embrace cross-modality for even broader domains and tasks."
28,CVPR,Seeing With Sound: Long-range Acoustic Beamforming for Multimodal Scene Understanding ,"[u'Praneeth Chakravarthula (Department of Computer Science, Princeton University) ', u' Jim Aldon D\u2019Souza (Torc Robotics) ', u' Ethan Tseng (Princeton University) ', u' Joe Bartusek (Columbia University) ', u' Felix Heide (Department of Computer Science, Princeton University)']","Department of Computer Science, Princeton University","Existing autonomous vehicles primarily use sensors that rely on electromagnetic waves which are undisturbed in good environmental conditions but can suffer in adverse scenarios, such as low light or for objects with low reflectance. Moreover, only objects in direct line-of-sight are typically detected by these existing methods. Acoustic pressure waves emanating from road users do not share these limitations. However, such signals are typically ignored in automotive perception because they suffer from low spatial resolution and lack directional information. In this work, we introduce long-range acoustic beamforming of pressure waves from noise directly produced by automotive vehicles in-the-wild as a complementary sensing modality to traditional optical sensor approaches for detection of objects in dynamic traffic environments. To this end, we introduce the first multimodal long-range acoustic beamforming dataset. We propose a neural aperture expansion method for beamforming and we validate its utility for multimodal automotive object detection. We validate the benefit of adding sound detections to existing RGB cameras in challenging automotive scenarios, where camera-only approaches fail or do not deliver the ultra-fast rates of pressure sensors."
29,CVPR,FlatFormer: Flattened Window Attention for Efficient Point Cloud Transformer ,"[u'Zhijian Liu (Massachusetts Institute of Technology) ', u' Xinyu Yang (Shanghai Jiaotong University) ', u' Haotian Tang (Massachusetts Institute of Technology) ', u' Shang Yang (Massachusetts Institute of Technology) ', u' Song Han (Massachusetts Institute of Technology)']",Massachusetts Institute of Technology,"Transformer, as an alternative to CNN, has been proven effective in many modalities (e.g., texts and images). For 3D point cloud transformers, existing efforts focus primarily on pushing their accuracy to the state-of-the-art level. However, their latency lags behind sparse convolution-based models (3x slower), hindering their usage in resource-constrained, latency-sensitive applications (such as autonomous driving). This inefficiency comes from point clouds' sparse and irregular nature, whereas transformers are designed for dense, regular workloads. This paper presents FlatFormer to close this latency gap by trading spatial proximity for better computational regularity. We first flatten the point cloud with window-based sorting and partition points into groups of equal sizes rather than windows of equal shapes. This effectively avoids expensive structuring and padding overheads. We then apply self-attention within groups to extract local features, alternate sorting axis to gather features from different directions, and shift windows to exchange features across groups. FlatFormer delivers state-of-the-art accuracy on Waymo Open Dataset with 4.6x speedup over (transformer-based) SST and 1.4x speedup over (sparse convolutional) CenterPoint. This is the first point cloud transformer that achieves real-time performance on edge GPUs and is faster than sparse convolutional methods while achieving on-par or even superior accuracy on large-scale benchmarks."
30,CVPR,Multi-Concept Customization of Text-to-Image Diffusion ,"[u'Nupur Kumari (None) ', u' Bingliang Zhang (Tsinghua University, Tsinghua University) ', u' Richard Zhang (Adobe Systems) ', u' Eli Shechtman (Adobe) ', u' Jun-Yan Zhu (Carnegie Mellon University)']",Carnegie Mellon University,"While generative models produce high-quality images of concepts learned from a large-scale database, a user often wishes to synthesize instantiations of their own concepts (for example, their family, pets, or items). Can we teach a model to quickly acquire a new concept, given a few examples? Furthermore, can we compose multiple new concepts together? We propose Custom Diffusion, an efficient method for augmenting existing text-to-image models. We find that only optimizing a few parameters in the text-to-image conditioning mechanism is sufficiently powerful to represent new concepts while enabling fast tuning ( 6 minutes). Additionally, we can jointly train for multiple concepts or combine multiple fine-tuned models into one via closed-form constrained optimization. Our fine-tuned model generates variations of multiple new concepts and seamlessly composes them with existing concepts in novel settings. Our method outperforms or performs on par with several baselines and concurrent works in both qualitative and quantitative evaluations, while being memory and computationally efficient."
31,CVPR,SmartBrush: Text and Shape Guided Object Inpainting With Diffusion Model ,"[u'Shaoan Xie (Carnegie Mellon University) ', u' Zhifei Zhang (Adobe Research) ', u' Zhe Lin (Adobe Research) ', u' Tobias Hinz (Adobe Systems) ', u' Kun Zhang (Carnegie Mellon University)']",Carnegie Mellon University,"Generic image inpainting aims to complete a corrupted image by borrowing surrounding information, which barely generates novel content. By contrast, multi-modal inpainting provides more flexible and useful controls on the inpainted content, e.g., a text prompt can be used to describe an object with richer attributes, and a mask can be used to constrain the shape of the inpainted object rather than being only considered as a missing area. We propose a new diffusion-based model named SmartBrush for completing a missing region with an object using both text and shape-guidance. While previous work such as DALLE-2 and Stable Diffusion can do text-guided inapinting they do not support shape guidance and tend to modify background texture surrounding the generated object. Our model incorporates both text and shape guidance with precision control. To preserve the background better, we propose a novel training and sampling strategy by augmenting the diffusion U-net with object-mask prediction. Lastly, we introduce a multi-task training strategy by jointly training inpainting with text-to-image generation to leverage more training data. We conduct extensive experiments showing that our model outperforms all baselines in terms of visual quality, mask controllability, and background preservation."
32,CVPR,Critical Learning Periods for Multisensory Integration in Deep Networks ,"[u'Michael Kleinman (University of California, Los Angeles) ', u' Alessandro Achille (California Institute of Technology) ', u' Stefano Soatto (AWS)']",AWS,"We show that the ability of a neural network to integrate information from diverse sources hinges critically on being exposed to properly correlated signals during the early phases of training. Interfering with the learning process during this initial stage can permanently impair the development of a skill, both in artificial and biological systems where the phenomenon is known as a critical learning period. We show that critical periods arise from the complex and unstable early transient dynamics, which are decisive of final performance of the trained system and their learned representations. This evidence challenges the view, engendered by analysis of wide and shallow networks, that early learning dynamics of neural networks are simple, akin to those of a linear model. Indeed, we show that even deep linear networks exhibit critical learning periods for multi-source integration, while shallow networks do not. To better understand how the internal representations change according to disturbances or sensory deficits, we introduce a new measure of source sensitivity, which allows us to track the inhibition and integration of sources during training. Our analysis of inhibition suggests cross-source reconstruction as a natural auxiliary training objective, and indeed we show that architectures trained with cross-sensor reconstruction objectives are remarkably more resilient to critical periods. Our findings suggest that the recent success in self-supervised multi-modal training compared to previous supervised efforts may be in part due to more robust learning dynamics and not solely due to better architectures and/or more data."
33,CVPR,Multi-Sensor Large-Scale Dataset for Multi-View 3D Reconstruction ,"[u'Oleg Voynov (Skolkovo Institute of Science and Technology) ', u' Gleb Bobrovskikh (Skolkovo Institute of Science and Technology) ', u' Pavel Karpyshev (Skolkovo Institute of Science and Technology) ', u' Saveliy Galochkin (Skoltech) ', u' Andrei-Timotei Ardelean (Friedrich-Alexander Universit\xe4t Erlangen-N\xfcrnberg) ', u' Arseniy Bozhenko (Skolkovo Institute of Science and Technology) ', u' Ekaterina Karmanova (Skolkovo Institute of Science and Technology ) ', u' Pavel Kopanev (Skolkovo Institute of Science and Technology) ', u' Yaroslav Labutin-Rymsho (Moscow Engineering Physics Institute) ', u' Ruslan Rakhimov (Skolkovo Institute of Science and Technology) ', u' Aleksandr Safin (Skolkovo Institute of Science and Technology) ', u' Valerii Serpiva (Skoltech Institute of Science and Technology) ', u' Alexey Artemov (Technische Universit\xe4t M\xfcnchen) ', u' Evgeny Burnaev (Skolkovo Institute of Science and Technology) ', u' Dzmitry Tsetserukou (Skoltech) ', u' Denis Zorin (New York University)']",New York University,"We present a new multi-sensor dataset for multi-view 3D surface reconstruction. It includes registered RGB and depth data from sensors of different resolutions and modalities: smartphones, Intel RealSense, Microsoft Kinect, industrial cameras, and structured-light scanner. The scenes are selected to emphasize a diverse set of material properties challenging for existing algorithms. We provide around 1.4 million images of 107 different scenes acquired from 100 viewing directions under 14 lighting conditions. We expect our dataset will be useful for evaluation and training of 3D reconstruction algorithms and for related tasks. The dataset is available at skoltech3d.appliedai.tech."
34,CVPR,Fast Monocular Scene Reconstruction With Global-Sparse Local-Dense Grids ,"[u'Wei Dong (Carnegie Mellon University) ', u' Christopher Choy (Stanford University) ', u' Charles Loop (NVIDIA Researxh) ', u' Or Litany (NVIDIA / Technion) ', u' Yuke Zhu (University of Texas - Austin) ', u' Anima Anandkumar (California Institute of Technology)']",California Institute of Technology,"Indoor scene reconstruction from monocular images has long been sought after by augmented reality and robotics developers. Recent advances in neural field representations and monocular priors have led to remarkable results in scene-level surface reconstructions. The reliance on Multilayer Perceptrons (MLP), however, significantly limits speed in training and rendering. In this work, we propose to directly use signed distance function (SDF) in sparse voxel block grids for fast and accurate scene reconstruction without MLPs. Our globally sparse and locally dense data structure exploits surfaces' spatial sparsity, enables cache-friendly queries, and allows direct extensions to multi-modal data such as color and semantic labels. To apply this representation to monocular scene reconstruction, we develop a scale calibration algorithm for fast geometric initialization from monocular depth priors. We apply differentiable volume rendering from this initialization to refine details with fast convergence. We also introduce efficient high-dimensional Continuous Random Fields (CRFs) to further exploit the semantic-geometry consistency between scene objects. Experiments show that our approach is 10x faster in training and 100x faster in rendering while achieving comparable accuracy to state-of-the-art neural implicit methods."
35,CVPR,LiDAR-in-the-Loop Hyperparameter Optimization ,"[u'F\xe9lix Goudreault (Universit\xe9 de Montr\xe9al) ', u' Dominik Scheuble (Universit\xe4t des Saarlandes) ', u' Mario Bijelic (Princeton University) ', u' Nicolas Robidoux (Algolux) ', u' Felix Heide (Department of Computer Science, Princeton University)']","Department of Computer Science, Princeton University","LiDAR has become a cornerstone sensing modality for 3D vision. LiDAR systems emit pulses of light into the scene, take measurements of the returned signal, and rely on hardware digital signal processing (DSP) pipelines to construct 3D point clouds from these measurements. The resulting point clouds output by these DSPs are input to downstream 3D vision models -- both, in the form of training datasets or as input at inference time. Existing LiDAR DSPs are composed of cascades of parameterized operations; modifying configuration parameters results in significant changes in the point clouds and consequently the output of downstream methods. Existing methods treat LiDAR systems as fixed black boxes and construct downstream task networks more robust with respect to measurement fluctuations. Departing from this approach, the proposed method directly optimizes LiDAR sensing and DSP parameters for downstream tasks. To investigate the optimization of LiDAR system parameters, we devise a realistic LiDAR simulation method that generates raw waveforms as input to a LiDAR DSP pipeline. We optimize LiDAR parameters for both 3D object detection IoU losses and depth error metrics by solving a nonlinear multi-objective optimization problem with a 0th-order stochastic algorithm. For automotive 3D object detection models, the proposed method outperforms manual expert tuning by 39.5% mean Average Precision (mAP)."
36,CVPR,Policy Adaptation From Foundation Model Feedback ,"[u'Yuying Ge (University of Hong Kong) ', u' Annabella Macaluso (University of California, San Diego) ', u' Li Erran Li (AWS AI, Amazon) ', u' Ping Luo (The University of Hong Kong) ', u' Xiaolong Wang (UCSD)']",UCSD,"Recent progress on vision-language foundation models have brought significant advancement to building general-purpose robots. By using the pre-trained models to encode the scene and instructions as inputs for decision making, the instruction-conditioned policy can generalize across different objects and tasks. While this is encouraging, the policy still fails in most cases given an unseen task or environment. In this work, we propose Policy Adaptation from Foundation model Feedback (PAFF). When deploying the trained policy to a new task or a new environment, we first let the policy play with randomly generated instructions to record the demonstrations. While the execution could be wrong, we can use the pre-trained foundation models to provide feedback to relabel the demonstrations. This automatically provides new pairs of demonstration-instruction data for policy fine-tuning. We evaluate our method on a broad range of experiments with the focus on generalization on unseen objects, unseen tasks, unseen environments, and sim-to-real transfer. We show PAFF improves baselines by a large margin in all cases."
37,CVPR,Ambiguous Medical Image Segmentation Using Diffusion Models ,"[u'Aimon Rahman (None) ', u' Jeya Maria Jose Valanarasu (Johns Hopkins University) ', u' Ilker Hacihaliloglu (University of British Columbia) ', u' Vishal M. Patel (Johns Hopkins University)']",Johns Hopkins University,"Collective insights from a group of experts have always proven to outperform an individual's best diagnostic for clinical tasks. For the task of medical image segmentation, existing research on AI-based alternatives focuses more on developing models that can imitate the best individual rather than harnessing the power of expert groups. In this paper, we introduce a single diffusion model-based approach that produces multiple plausible outputs by learning a distribution over group insights. Our proposed model generates a distribution of segmentation masks by leveraging the inherent stochastic sampling process of diffusion using only minimal additional learning. We demonstrate on three different medical image modalities- CT, ultrasound, and MRI that our model is capable of producing several possible variants while capturing the frequencies of their occurrences. Comprehensive results show that our proposed approach outperforms existing state-of-the-art ambiguous segmentation networks in terms of accuracy while preserving naturally occurring variation. We also propose a new metric to evaluate the diversity as well as the accuracy of segmentation predictions that aligns with the interest of clinical practice of collective insights. Implementation code will be released publicly after the review process."
38,CVPR,SpaText: Spatio-Textual Representation for Controllable Image Generation ,"[u'Omri Avrahami (The Hebrew University of Jerusalem) ', u' Thomas Hayes (Meta) ', u' Oran Gafni (Meta AI) ', u' Sonal Gupta (Facebook) ', u' Yaniv Taigman (Facebook) ', u' Devi Parikh (Meta / Georgia Tech) ', u' Dani Lischinski (The Hebrew University of Jerusalem, Israel) ', u' Ohad Fried (Reichman University) ', u' Xi Yin (Facebook)']",Facebook,"Recent text-to-image diffusion models are able to generate convincing results of unprecedented quality. However, it is nearly impossible to control the shapes of different regions/objects or their layout in a fine-grained fashion. Previous attempts to provide such controls were hindered by their reliance on a fixed set of labels. To this end, we present SpaText --- a new method for text-to-image generation using open-vocabulary scene control. In addition to a global text prompt that describes the entire scene, the user provides a segmentation map where each region of interest is annotated by a free-form natural language description. Due to lack of large-scale datasets that have a detailed textual description for each region in the image, we choose to leverage the current large-scale text-to-image datasets and base our approach on a novel CLIP-based spatio-textual representation, and show its effectiveness on two state-of-the-art diffusion models: pixel-based and latent-based. In addition, we show how to extend the classifier-free guidance method in diffusion models to the multi-conditional case and present an alternative accelerated inference algorithm. Finally, we offer several automatic evaluation metrics and use them, in addition to FID scores and a user study, to evaluate our method and show that it achieves state-of-the-art results on image generation with free-form textual scene control."
39,CVPR,Dynamic Inference With Grounding Based Vision and Language Models ,"[u'Burak Uzkent (Stanford University) ', u' Amanmeet Garg (Amazon) ', u' Wentao Zhu (Amazon) ', u' Keval Doshi (Amazon) ', u' Jingru Yi (Rutgers University) ', u' Xiaolong Wang (Amazon Prime Video) ', u' Mohamed Omar (Amazon)']",Amazon,"Transformers have been recently utilized for vision and language tasks successfully. For example, recent image and language models with more than 200M parameters have been proposed to learn visual grounding in the pre-training step and show impressive results on downstream vision and language tasks. On the other hand, there exists a large amount of computational redundancy in these large models which skips their run-time efficiency. To address this problem, we propose dynamic inference for grounding based vision and language models conditioned on the input image-text pair. We first design an approach to dynamically skip multihead self-attention and feed forward network layers across two backbones and multimodal network. Additionally, we propose dynamic token pruning and fusion for two backbones. In particular, we remove redundant tokens at different levels of the backbones and fuse the image tokens with the language tokens in an adaptive manner. To learn policies for dynamic inference, we train agents using reinforcement learning. In this direction, we replace the CNN backbone in a recent grounding-based vision and language model, MDETR, with a vision transformer and call it ViTMDETR. Then, we apply our dynamic inference method to ViTMDETR, called D-ViTDMETR, and perform experiments on image-language tasks. Our results show that we can improve the run-time efficiency of the state-of-the-art models MDETR and GLIP by up to 50% on Referring Expression Comprehension and Segmentation, and VQA with only maximum 0.3% accuracy drop."
40,CVPR,Analyzing Physical Impacts Using Transient Surface Wave Imaging ,"[u'Tianyuan Zhang (Carnegie Mellon University) ', u' Mark Sheinin (Carnegie Mellon University) ', u' Dorian Chan (Carnegie Mellon University) ', u' Mark Rau (Stanford University) ', u' Matthew O\u2019Toole (Carnegie Mellon University) ', u' Srinivasa G. Narasimhan (Carnegie Mellon University)']",Carnegie Mellon University,"The subtle vibrations on an object's surface contain information about the object's physical properties and its interaction with the environment. Prior works imaged surface vibration to recover the object's material properties via modal analysis, which discards the transient vibrations propagating immediately after the object is disturbed. Conversely, prior works that captured transient vibrations focused on recovering localized signals (e.g., recording nearby sound sources), neglecting the spatiotemporal relationship between vibrations at different object points. In this paper, we extract information from the transient surface vibrations simultaneously measured at a sparse set of object points using the dual-shutter camera described by Sheinin[31]. We model the geometry of an elastic wave generated shortly after an object's surface is disturbed (e.g., a knock or a footstep), and use the model to localize the disturbance source for various materials (e.g., wood, plastic, tile). We also show that transient object vibrations contain additional cues about the impact force and the impacting object's material properties. We demonstrate our approach in applications like localizing the strikes of a ping-pong ball on a table mid-play and recovering the footsteps' locations by imaging the floor vibrations they create."
41,CVPR,Mobile User Interface Element Detection via Adaptively Prompt Tuning ,"[u'Zhangxuan Gu (None) ', u' Zhuoer Xu (Nanjing University) ', u' Haoxing Chen (Nanjing University) ', u' Jun Lan (Shanghai Jiaotong University) ', u' Changhua Meng (Nanjing University) ', u' Weiqiang Wang (University of Southern California)']",University of Southern California,"Recent object detection approaches rely on pretrained vision-language models for image-text alignment. However, they fail to detect the Mobile User Interface (MUI) element since it contains additional OCR information, which describes its content and function but is often ignored. In this paper, we develop a new MUI element detection dataset named MUI-zh and propose an Adaptively Prompt Tuning (APT) module to take advantage of discriminating OCR information. APT is a lightweight and effective module to jointly optimize category prompts across different modalities. For every element, APT uniformly encodes its visual features and OCR descriptions to dynamically adjust the representation of frozen category prompts. We evaluate the effectiveness of our plug-and-play APT upon several existing CLIP-based detectors for both standard and open-vocabulary MUI element detection. Extensive experiments show that our method achieves considerable improvements on two datasets. The datasets is available at github.com/antmachineintelligence/MUI-zh."
42,CVPR,iQuery: Instruments As Queries for Audio-Visual Sound Separation ,"[u'Jiaben Chen (University of California, San Diego) ', u' Renrui Zhang (MMLab of CUHK &amp;amp; Shanghai AI Laboratory) ', u' Dongze Lian (National University of Singapore) ', u' Jiaqi Yang (ShanghaiTech University) ', u' Ziyao Zeng (ShanghaiTech University) ', u' Jianbo Shi (None)']",None,"Current audio-visual separation methods share a standard architecture design where an audio encoder-decoder network is fused with visual encoding features at the encoder bottleneck. This design confounds the learning of multi-modal feature encoding with robust sound decoding for audio separation. To generalize to a new instrument, one must fine-tune the entire visual and audio network for all musical instruments. We re-formulate the visual-sound separation task and propose Instruments as Queries (iQuery) with a flexible query expansion mechanism. Our approach ensures cross-modal consistency and cross-instrument disentanglement. We utilize ""visually named"" queries to initiate the learning of audio queries and use cross-modal attention to remove potential sound source interference at the estimated waveforms. To generalize to a new instrument or event class, drawing inspiration from the text-prompt design, we insert additional queries as audio prompts while freezing the attention mechanism. Experimental results on three benchmarks demonstrate that our iQuery improves audio-visual sound source separation performance. Code is available at https://github.com/JiabenChen/iQuery."
43,CVPR,SceneComposer: Any-Level Semantic Image Synthesis ,"[u'Yu Zeng (None) ', u' Zhe Lin (Adobe Research) ', u' Jianming Zhang (Adobe Systems) ', u' Qing Liu (Adobe Systems) ', u' John Collomosse (University of Surrey) ', u' Jason Kuen (Adobe Research) ', u' Vishal M. Patel (Johns Hopkins University)']",Johns Hopkins University,"We propose a new framework for conditional image synthesis from semantic layouts of any precision levels, ranging from pure text to a 2D semantic canvas with precise shapes. More specifically, the input layout consists of one or more semantic regions with free-form text descriptions and adjustable precision levels, which can be set based on the desired controllability. The framework naturally reduces to text-to-image (T2I) at the lowest level with no shape information, and it becomes segmentation-to-image (S2I) at the highest level. By supporting the levels in-between, our framework is flexible in assisting users of different drawing expertise and at different stages of their creative workflow. We introduce several novel techniques to address the challenges coming with this new setup, including a pipeline for collecting training data; a precision-encoded mask pyramid and a text feature map representation to jointly encode precision level, semantics, and composition information; and a multi-scale guided diffusion model to synthesize images. To evaluate the proposed method, we collect a test dataset containing user-drawn layouts with diverse scenes and styles. Experimental results show that the proposed method can generate high-quality images following the layout at given precision, and compares favorably against existing methods. Project page https://zengxianyu.github.io/scenec/"
44,CVPR,DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation ,"[u'Nataniel Ruiz (Boston University) ', u' Yuanzhen Li (Massachusetts Institute of Technology) ', u' Varun Jampani (Google Research) ', u' Yael Pritch (Google Research) ', u' Michael Rubinstein (Google) ', u' Kfir Aberman (Google)']",Google,"Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. In this work, we present a new approach for ""personalization"" of text-to-image diffusion models. Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views and lighting conditions that do not appear in the reference images. We apply our technique to several previously-unassailable tasks, including subject recontextualization, text-guided view synthesis, and artistic rendering, all while preserving the subject's key features. We also provide a new dataset and evaluation protocol for this new task of subject-driven generation. Project page: https://dreambooth.github.io/"
45,CVPR,Unsupervised Sampling Promoting for Stochastic Human Trajectory Prediction ,"[u'Guangyi Chen (MBZUAI, CMU) ', u' Zhenhao Chen (None) ', u' Shunxing Fan (MBZUAI) ', u' Kun Zhang (Carnegie Mellon University)']",Carnegie Mellon University,"The indeterminate nature of human motion requires trajectory prediction systems to use a probabilistic model to formulate the multi-modality phenomenon and infer a finite set of future trajectories. However, the inference processes of most existing methods rely on Monte Carlo random sampling, which is insufficient to cover the realistic paths with finite samples, due to the long tail effect of the predicted distribution. To promote the sampling process of stochastic prediction, we propose a novel method, called BOsampler, to adaptively mine potential paths with Bayesian optimization in an unsupervised manner, as a sequential design strategy in which new prediction is dependent on the previously drawn samples. Specifically, we model the trajectory sampling as a Gaussian process and construct an acquisition function to measure the potential sampling value. This acquisition function applies the original distribution as prior and encourages exploring paths in the long-tail region. This sampling method can be integrated with existing stochastic predictive models without retraining. Experimental results on various baseline methods demonstrate the effectiveness of our method. The source code is released in this link."
